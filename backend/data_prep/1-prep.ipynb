{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cf5c96",
   "metadata": {},
   "source": [
    "Here, we get relevant data and parse a html converted from a pdf found at the url: of the top chinese characters:   \n",
    "\n",
    "http://www.zein.se/patrick/3000char.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b358aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import hanzidentifier\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# Suppress just SettingWithCopyWarningda\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a52bf8",
   "metadata": {},
   "source": [
    "### Part 1: get initial data of simpilifed and traditional characters, along with phrases/multi-character words in traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188067ee",
   "metadata": {},
   "source": [
    "Loop through the tables of the html. With the exception of the first one, they follow a predictable pattern of definition, words from by multiple characters. \n",
    "\n",
    "We create an initial datasets: one with characters only (both simplifed and traditional), together another with multiple characters and their pronunciations. The character category(cat) is the index of the character (used for ranking relatively difficulty).\n",
    "\n",
    "For alternative characters (mainly numbers) we only show it for the single character word\n",
    "\n",
    "Special care is used to ensure merged traditional forms can be separated based on the word and meaning, such as (幹乾=>干)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd20796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our key helper function to parse definitions and pronunciations apart given the raw main character form, etc\n",
    "#returns a list of lists with the order [\"cat\", \"word/character\", \"pronunciation\", \"definition\", \"code\",\"alt\"]. Code is if the word uses traditional/simplified (s,t,a). More contains extra information (in few cases where there is an alternative character)\n",
    "# since many words may have compound simplified characters in this list, we will later need to convert again for the words that \n",
    "#note pronunciation is the the pronunciation of the character in the word\n",
    "# are desired to only be written in traditional.\n",
    "#Therefore, each row will be written twice, one for traditional one for simplified\n",
    "#for single characters, we use the first pronunciation listed\n",
    "\n",
    "def getbetween(input, start, end): #helper function to get the string in between two symbols (the first matching abtter)\n",
    "    startind = input.find(start)\n",
    "    #if the first one is not found, return empty:\n",
    "    if startind == -1: \n",
    "        return \"\"\n",
    "    #narrow down input\n",
    "    input = input[startind+len(start):]\n",
    "    endind = input.find(end)\n",
    "    #if the first one is not found (but start is found), return the string starting from the end \n",
    "    if endind == -1:\n",
    "        return input\n",
    "    return input[:endind].strip() #otherwise, return \n",
    "\n",
    "#current helper function to get all text between characters (a list). Add space to the list, to not include whitespace\n",
    "def getall_betweenchars(charlist, text):\n",
    "    #if the length of the list is empty\n",
    "    if len(charlist) == 0:\n",
    "        return \"\" #empty string\n",
    "    if len(charlist) == 1: #if there is only element, return the only one\n",
    "        return charlist[0] #empty string\n",
    "    #get the max and lowest indices \n",
    "    charlist.append(\" \")\n",
    "    max = 0 \n",
    "    max_string = \"\" #string with the min index\n",
    "    min = 1000000 #absurdly large\n",
    "    min_string = \"\" \n",
    "    for char in charlist:\n",
    "        #find the index of the charcter in the, text\n",
    "        ind = text.find(char)\n",
    "        if ind > max:\n",
    "            max = ind\n",
    "            max_string = char\n",
    "        if ind < min:\n",
    "            min = ind\n",
    "            min_string = char\n",
    "    l2 = len(max_string)\n",
    "    return text[min:max+l2]\n",
    "\n",
    "def get_wordList(main, des, cat):\n",
    "    ret = [] #list to return\n",
    "\n",
    "    simp = main.split(\"(\")[0] #get the simplified form\n",
    "    alt = getbetween(main,\"(A\",\")\") #set as empty string initially for the alternative form #check for alternative form, by default if there is non, this function should return an empty string \n",
    "    #the the traditional form, if any \n",
    "    trad = getbetween(main,\"(F\",\")\")\n",
    "    if trad == \"\": #if function is empty, get to be the same as the simplified character\n",
    "        trad = simp\n",
    "    print(\"current simp. :\"+simp+\", current trad. :\"+trad)\n",
    "    frags = des.split(\"[\") #split on [\n",
    "    #iterate through diffenent fragments with pronunciations and descriptions within the definition\n",
    "    #count the number of total definitions\n",
    "    i = 0\n",
    "    for frag in frags:\n",
    "        #get the separate pronunciations\n",
    "        pro = frag.split(\"]\")[0]\n",
    "        print(\"current pronunciation:\"+pro)\n",
    "        #if not empty\n",
    "        if len(pro) > 0:\n",
    "            #get rid of the part that is not part of the pronunciaiton\n",
    "            defis = frag.replace(f\"{pro}]\", \"\")\n",
    "            #print(\"pronunciation:\", pro, \",definitions:\", defis)\n",
    "            defis = defis.split(\";\")#get separate defintions\n",
    "            defis = [x for x in defis if x.strip()] #remove empty strings\n",
    "#replace empty strings\n",
    "            for d in range(0,len(defis)):\n",
    "                defi =  defis[d].strip() #get a certain definition, with leading/ training whitespace remove. def is the simplified def. \n",
    "                print(\"current def:\", defi)\n",
    "                \n",
    "                newTrad = False #store if at least 1 new trad. character is found\n",
    "                #check for potentially multiple traditional characters\n",
    "                def_parts = defi.split(\", \")\n",
    "                exclude = [] #append for parts to exclude in the traditional definitions (since we have to substract the separate traditional definition in this case)\n",
    "                for part in def_parts:\n",
    "                    print(\"current part:\", part)\n",
    "                    trad2 = getbetween(part,\"(F\",\")\") #modify the fragment string\n",
    "                    if len(trad2) != 0: #if function is not empty, get to be the same as the simplified character. In this case we have a in line traditional character\n",
    "                        newTrad = True\n",
    "                        minidef = getbetween(part ,trad2, \", \")\n",
    "                        minidef = minidef.split(\")\")[1].strip() #get the minidef (specialized definition between the new character and commas and if any)\n",
    "                        nestchar = trad2.split(\",\")#for the few cases where there are multiple nested characters like (裏,裡), we set the 2nd one to be the variant\n",
    "                        mainchar = nestchar[0]\n",
    "                        #new_alternative character\n",
    "                        new_alt = \"\"\n",
    "                        print(\"minidef:\", minidef, \", in def:\", defi, \"with character:\", mainchar, \", and alternative:\", new_alt)\n",
    "                        if len(nestchar) > 1: new_alt = nestchar[1] #assign the variant to be the 2nd one\n",
    "                        #in the case we have a subdef, we append an extra traditional entry \n",
    "                        new1 = [cat, mainchar, pro, minidef, \"t\",new_alt] #\"cat\", \"word/charcter\", \"pronunciation\", \"definition\", \"code\",\"alt\"\n",
    "                        ret.append(new1)\n",
    "                    \n",
    "                        ex_str = f\", (F {trad2}) {minidef}\"\n",
    "                        exclude.append(ex_str) #string to replace in the original definition for the traditional definition\n",
    "                traddef = defi  #create new traditional definition excluding the existing one\n",
    "                print(\"exclusion list:\", exclude)\n",
    "                for ex in exclude:\n",
    "                    traddef = traddef.replace(ex, \"\")\n",
    "\n",
    "                \n",
    "                chinese_pattern  = re.compile(r'[\\u4E00-\\u9FFF]+') #regex to get list of matching characters \n",
    "                match = chinese_pattern.findall(traddef)\n",
    "                han = getall_betweenchars(match, traddef)   #helper to get all chars between the list of matching characters \n",
    "                print(\"found subcharacter:\"+han+\"|\")\n",
    "                if len(han)>1 and i!=0: #if the def definition contains multiple chinese characters, we do extra processing to extract the characters, and if this is not a new trad \n",
    "                    charind = defi.find(han) #find last index\n",
    "                    char = han.strip() # the character for the word \n",
    "                    defi = defi[charind+len(han):] #modify the definition to be the remaining part\n",
    "                    new1 = [cat, char, pro, defi, \"s\",alt] #\"cat\", \"word/charcter\", \"pronunciation\", \"definition\", \"code\",\"alt\"\n",
    "                    new2 = [cat, char, pro, traddef, \"t\",alt] #for now\n",
    "                    ret.append(new1)\n",
    "                    ret.append(new2)\n",
    "                    #in this case, we append as so \n",
    "                    print(\"ran if\"+str(new2))\n",
    "                elif newTrad==False: #assume that there is only one character to the definition, and no traditional characters found\n",
    "                    new1 = [cat, simp, pro, defi, \"s\",alt] #\"cat\", \"word/charcter\", \"pronunciation\", \"definition\", \"code\",\"alt\"\n",
    "                    new2 = [cat, trad, pro, defi, \"t\",alt]\n",
    "                    ret.append(new1)\n",
    "                    ret.append(new2)\n",
    "                    print(\"ran elif 1\"+str(new2))\n",
    "                else: \n",
    "                    print(\"ran else (diff traditional and simplified definition)\")\n",
    "\n",
    "                    new1 = [cat, simp, pro, defi, \"s\",alt] #simplified definotion is as usual\"\n",
    "                    new2 = [cat, trad, pro, traddef, \"t\",alt]\n",
    "                    ret.append(new1)\n",
    "                    ret.append(new2)\n",
    "                #print(\"character:\"+char+\", definition:\"+defi)\n",
    "                i+= 1\n",
    "\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"./main_char_set-converted.html\"\n",
    "\n",
    "with open(link, 'r') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "tables = soup.find_all(\"table\")  \n",
    "grand_list = [] #list used to create the dataframe in the end\n",
    "#count the number of tables \n",
    "index = 0;\n",
    "for table in tables:\n",
    "    rows = table.find_all(\"tr\") #extract all tables\n",
    "    for row in rows: \n",
    "        # Extract all cells (td or th)\n",
    "        cols = row.find_all([\"td\"]) #print the number of cols (exclude the header) \n",
    "        #valid character columns have 3 columns \n",
    "        if len(cols) == 3:\n",
    "            #get the number column \n",
    "            col1 = cols[0].get_text()\n",
    "            #get the char column \n",
    "            col2 = cols[1].get_text()\n",
    "            #get description column, this is complicated and is where we split the characters from the words, using a helper function\n",
    "            col3 = cols[2].get_text()\n",
    "            #call our key helper function that retruns a list to be appended to the grand_list \n",
    "            res = get_wordList(col2, col3, cat=index)\n",
    "            grand_list.append(res)\n",
    "            index += 1\n",
    "print(\"total number of characters listed:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grand_list))\n",
    "grand_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59fcad4",
   "metadata": {},
   "source": [
    "As expected, the list has 2431 subcomponents, but to effectively create the dataframe we need to remove a layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5de96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "for l1 in grand_list:\n",
    "    for l2 in l1:\n",
    "        print(l2)\n",
    "        master_list.append(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ee356",
   "metadata": {},
   "source": [
    "Saving to a new csv file. First, remove the last two and first two entries from the, its clean its garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d40f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = master_list[2:len(master_list)-2]\n",
    "for l in master_list:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075e3560",
   "metadata": {},
   "source": [
    "We now save to a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52260fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.DataFrame(master_list, columns=[\"cat\", \"word/character\", \"pronunciation\", \"definition\", \"code\",\"alt\"])\n",
    "data.to_csv(\"main_list.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813b494",
   "metadata": {},
   "source": [
    "### Part 2: Quality repairs\n",
    "\n",
    "Creating a map of simplified to traditional characters so we can effectively edit multicharacter words here. Note that this is map from simplified characters to a list of traditional ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate all rows of single characters \n",
    "single_characters = data[data[\"word/character\"].str.len() == 1]\n",
    "print(len(single_characters))\n",
    "single_characters.head(n=1)\n",
    "#save \n",
    "single_characters.to_csv(\"character_list.csv\", index=False)\n",
    "#iteration based on category\n",
    "cats = single_characters[\"cat\"].unique()\n",
    "StoTmap = {} #character map\n",
    "\n",
    "for cat in cats:\n",
    "    subset =  single_characters[single_characters[\"cat\"] == cat]\n",
    "    subset = subset.drop_duplicates(subset=['word/character']) #get the unique character rows \n",
    "    char_list = set(subset[\"word/character\"]) #get the of characters\n",
    "    #the simplified character is the one where the code is \"s\"\n",
    "    simp = subset[subset[\"code\"]==\"s\"].iloc[0][\"word/character\"]\n",
    "    trad = list(subset[subset[\"code\"]==\"t\"][\"word/character\"]) #get list of purely traditional ch\n",
    "    if len(trad) == 0: #if the list of traditional characters is empty, then they must be the same\n",
    "        StoTmap[simp] = [simp]\n",
    "    else:\n",
    "        StoTmap[simp] = list(trad)\n",
    "for key in StoTmap:\n",
    "    print(key,\",\", StoTmap[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037c381",
   "metadata": {},
   "source": [
    "get the inverse map. this is easy since each traditional character maps to just one simplified one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TtoS = {}\n",
    "for key in StoTmap:\n",
    "    Tlist = StoTmap[key]\n",
    "    for Tchar in Tlist:\n",
    "        TtoS[Tchar] = key \n",
    "for key in TtoS:\n",
    "    print(key,\",\", StoTmap[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487628e1",
   "metadata": {},
   "source": [
    "Now, attempt to convert most compound words and definition characters to traditional automatically using this dictionary (for cases where the category is traditional). For cases of multiple mappings, we skip and print out a message highlighting the index to correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = StoTmap\n",
    "#store failed indices for the word/character column\n",
    "failed_chars = []\n",
    "#store failed indices for the definition column\n",
    "definition_chars = []\n",
    "def convert(data): #    \n",
    "    for index, row in data.iterrows():\n",
    "        #first convert characters \n",
    "        code = row[\"code\"]\n",
    "        word = row[\"word/character\"]\n",
    "        defi = row[\"definition\"]\n",
    "        #check if variables are strings or not\n",
    "        if (isinstance(defi, str) == False):\n",
    "            print(f\"definition at index {index} is not a string: {defi}, for char {word}\")\n",
    "            print(\"converted to:\", str(defi))\n",
    "\n",
    "        if code == \"t\":\n",
    "            #convert the word first \n",
    "            new_word = \"\" #new string \n",
    "            for char in str(word):\n",
    "                #check if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"failed to word/character column at index:\", index,\", with char:\", char, \"failing\", \", context is:\", word)\n",
    "                        failed_chars.append(index)\n",
    "                        new_word += char #assign back to old word in this case\n",
    "                    else: \n",
    "                        new_word += char_list[0] #increment \n",
    "                else:\n",
    "                    new_word += char #increment like usual \n",
    "            data.at[index,\"word/character\"] = new_word #assign \n",
    "\n",
    "            #then, convert the description characters. use the same character if possible\n",
    "            new_defi = \"\" #new string \n",
    "            for char in str(defi):\n",
    "                #check if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"failed to definition column at index:\", index,\", with char:\", char, \"failing\", \", context is:\", word)\n",
    "                        definition_chars.append(index)\n",
    "                        new_defi += char #increment like usual \n",
    "                    else: \n",
    "                        new_defi += char_list[0] #increment \n",
    "                else:\n",
    "                    new_defi += char #increment like usual \n",
    "            data.at[index,\"definition\"] = new_defi #assign \n",
    "\n",
    "    return data\n",
    "data = convert(data)\n",
    "data.to_csv(\"./main_list.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05098f",
   "metadata": {},
   "source": [
    "We then create a function to spot traditiona words and definitions that are not traditional characters (meaning one character that is in the simplified map cannot be easily converted because of the multiple-mapping issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7abbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(data): #    \n",
    "    for index, row in data.iterrows():\n",
    "        #first convert characters \n",
    "        code = row[\"code\"]\n",
    "        word = row[\"word/character\"]\n",
    "        defi = row[\"definition\"]\n",
    "\n",
    "        if code == \"t\":\n",
    "            #convert the word first \n",
    "            for char in str(word):\n",
    "                #check key if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"word/character column misses a traditional character at:\", index,\", char:\", char, f\", possibly mapping to: f{char_list}\") \n",
    "                        print(f\"\\t context is:\", word)\n",
    "                        failed_chars.append(index)\n",
    "            for char in str(defi):\n",
    "                #check if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"definition column misses a traditional character at:\", index,\", char:\", char, f\", possibly mapping to: f{char_list}\") \n",
    "                        print(f\"\\t context is:\", word)\n",
    "                        definition_chars.append(index)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./main_list.csv\")\n",
    "check(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047b299",
   "metadata": {},
   "source": [
    "Print out failed indices text for word/character column. From the outputs it is clear that the all failures are a single character, and so far words do not have their actual pronunciation, we aim to fix that here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2131da",
   "metadata": {},
   "source": [
    "Changes were manually done based on the output. We save to a new file name, main_final_list and read again and reindex.\n",
    "\n",
    "Many changes were made to delete duplicate entries caused by inconsistent formating ,etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f18397",
   "metadata": {},
   "source": [
    "### Part 3: final dataset export, more quality changes\n",
    "\n",
    "We now do more quality changed. re run this part on the new data until its fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00d3b7",
   "metadata": {},
   "source": [
    "First, get a new character map and match based on the changes. note now we dont drop based on both the character and pronunciation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./main_final_list.csv\"\n",
    "new_data = pd.read_csv(path)\n",
    "\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = new_data.columns[new_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "new_data.drop(columns=unnamed_cols, inplace=True)\n",
    "new_data = new_data.reindex()\n",
    "new_data.to_csv(path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate all rows of single characters \n",
    "single_characters = new_data[new_data[\"word/character\"].str.len() == 1]\n",
    "print(len(single_characters))\n",
    "single_characters.head(n=1)\n",
    "#save \n",
    "single_characters.to_csv(\"character_list.csv\", index=False)\n",
    "#iteration based on category\n",
    "cats = single_characters[\"cat\"].unique()\n",
    "StoTmap = {} #character map\n",
    "\n",
    "for cat in cats:\n",
    "    #get all characters \n",
    "    subset = single_characters[single_characters[\"cat\"] == cat]\n",
    "    #get the traditional subset first\n",
    "    trad_subset =  subset[subset[\"code\"] == \"t\"]\n",
    "    simp_subset =  subset[subset[\"code\"] == \"s\"]\n",
    "    trad_subset = trad_subset.drop_duplicates(subset=['word/character']) #get the unique character rows \n",
    "    simp_subset = simp_subset.drop_duplicates(subset=['word/character'])\n",
    "    \n",
    "    trads = list(set(trad_subset[\"word/character\"]))\n",
    "    simps = list(simp_subset[\"word/character\"])\n",
    "    simp = simps[0]\n",
    "    \n",
    "    StoTmap[simp] = list(trads)\n",
    "#print the map \n",
    "for key in StoTmap:\n",
    "    listc =  StoTmap[key]\n",
    "    print(key,\",\", StoTmap[key])\n",
    "    if (len(listc) == 0):\n",
    "        print(\"empty trad list for simp. character:\", key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2c519",
   "metadata": {},
   "source": [
    "Finding the inverse map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TtoS = {}\n",
    "for key in StoTmap:\n",
    "    Tlist = StoTmap[key]\n",
    "    for Tchar in Tlist:\n",
    "        TtoS[Tchar] = key \n",
    "for key in TtoS:\n",
    "    print(key,\",\", TtoS[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bee909",
   "metadata": {},
   "source": [
    "Note: checking for empty lists, we did further corrections to the main_final_list. The 5 Errors appear to be from the alternate characters not properly parsed. We then ran this part again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4cfb6",
   "metadata": {},
   "source": [
    "Check the data again using the same function but the new character map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = StoTmap\n",
    "#store failed indices for the word/character column\n",
    "failed_chars = []\n",
    "#store failed indices for the definition column\n",
    "definition_chars = []\n",
    "def check(data): #    \n",
    "    for index, row in data.iterrows():\n",
    "        #first convert characters \n",
    "        code = row[\"code\"]\n",
    "        word = row[\"word/character\"]\n",
    "        defi = row[\"definition\"]\n",
    "\n",
    "        if code == \"t\":\n",
    "            #convert the word first \n",
    "            for char in str(word):\n",
    "                #check key if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"word/character column misses a traditional character at:\", index,\", char:\", char, f\", possibly mapping to: f{char_list}\") \n",
    "                        print(f\"\\t context is:\", word)\n",
    "                        failed_chars.append(index)\n",
    "            for char in str(defi):\n",
    "                #check if in dict \n",
    "                if char in charmap:\n",
    "                    char_list = charmap[char]\n",
    "                    if (len(char_list)>1): \n",
    "                        print(\"definition column misses a traditional character at:\", index,\", char:\", char, f\", possibly mapping to: f{char_list}\") \n",
    "                        print(f\"\\t context is:\", word)\n",
    "                        definition_chars.append(index)\n",
    "\n",
    "    return\n",
    "check(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f66cb",
   "metadata": {},
   "source": [
    "finally, view rows with invalid characters, and manually repair altter (F: with a more clear traditional mark) and (S with a more clear simplified mark). Check for duplicate rows with the same pronunciation and character [noting many characters can have different pronuciation] (within the same category), then manually fix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function check if a string of chracters is all chinese\n",
    "def contains_chinese(text): \n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]'  \n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "#create function check if a single characer is all chinese or punctuation\n",
    "def char_chinese_or_punctuation(text): \n",
    "    if len(text) < 1:\n",
    "        print(\"must be a single string character\")\n",
    "        return False\n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]' \n",
    "    isChinese = bool(re.search(pattern, text))\n",
    "    isPronuncation = text in string.punctuation\n",
    "    return (isChinese or isPronuncation)\n",
    "\n",
    "def allChinese(text):\n",
    "    for char in text: \n",
    "        if char_chinese_or_punctuation(char) == False: #use function to check if a single characer is chiense\n",
    "            return False \n",
    "    return True\n",
    "print(allChinese(\"领导\"))\n",
    "print(allChinese(\"元宵,...节\"))\n",
    "print(allChinese(\"地支 dìzhī the Twelve Terrestrial Branches\"))\n",
    "\n",
    "#helper function, that find the help of a map from Traditional character in a string to simplified, attempts conversion of a string\n",
    "def toSimplified(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                rettext += map[char]\n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"trad char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext\n",
    "\n",
    "#helper function, that find the help of our custom map a map from simplified characters to traditional, attempts conversion of a string\n",
    "#handles cases where there are multiple mappings\n",
    "def toTraditional(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                #check the mapped list\n",
    "                if len(map[char]) == 1:\n",
    "                    rettext += map[char][0]\n",
    "                else: \n",
    "                    print(f\"simp char {char} has multiple possible traditional chars, use manual mapping, using the same character as the original text\")\n",
    "                    rettext += text[i] \n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"simp char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302f8ab",
   "metadata": {},
   "source": [
    "Checking function to convert appropriately to traditional or simplified, and running function appropriately, based on the output, we edit the file again, if conversion is not possible. We first check if we need to convert using a special python library. note that often times, it is fine if the definition of a simplified word/character contains traditional since it is giving extra information. However, all such errors in the word must be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963054b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing functions\n",
    "print(toSimplified(\"hi我不知道你在哪裏，在台灣嗎!\", TtoS))\n",
    "print(toTraditional(\"hi我不知道你在哪里在台湾吗!\", StoTmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_data.iterrows():\n",
    "    #first convert characters \n",
    "    code = str(row[\"code\"])\n",
    "    word = str(row[\"word/character\"])\n",
    "    defi = str(row[\"definition\"])\n",
    "    \n",
    "    new_word = word\n",
    "    new_def = defi\n",
    "    for char in word:\n",
    "        if code == \"s\" and contains_chinese(char): #run checks on all inerated \n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(\"error in word at index:\", index)\n",
    "                new_word = toSimplified(word, TtoS)\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(\"error in word at index:\", index)\n",
    "                new_word = toTraditional(word, StoTmap)\n",
    "    for char in defi:\n",
    "        if code == \"s\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(\"error in def at index:\", index)\n",
    "                new_def = toSimplified(defi, TtoS)\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(\"error in def at index:\", index)\n",
    "                new_def = toTraditional(defi, StoTmap)\n",
    "    #set with the new word if needed \n",
    "    #with with the new def if needed\n",
    "    if new_def != defi:\n",
    "        new_data.at[index, \"definition\"] = new_def\n",
    "        print(\"changed definition at:\", index)\n",
    "    if new_word != word: \n",
    "        new_data.at[index, \"word/character\"] = new_word\n",
    "        print(\"changed word/character at:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_data.iterrows():\n",
    "    #first convert characters \n",
    "    code = str(row[\"code\"])\n",
    "    word = str(row[\"word/character\"])\n",
    "    defi = str(row[\"definition\"])\n",
    "    \n",
    "    for char in word:\n",
    "        if code == \"s\" and contains_chinese(char): #run checks on all inerated \n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(f\"at word: {word} ({index})|{char} not simplified chinese\")\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(f\"at word: {word} ({index})|{char} not traditional chinese\")\n",
    "    for char in defi:\n",
    "        if code == \"s\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(f\"at def: {defi} ({index})|{char} not simplified chinese\")\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(f\"at def: {defi} ({index})|{char} not traditional chinese\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8f15a",
   "metadata": {},
   "source": [
    "Running until there is no output. indicating the problem has been fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_data.iterrows():\n",
    "    #first convert characters \n",
    "    code = row[\"code\"]\n",
    "    word = row[\"word/character\"]\n",
    "    defi = row[\"definition\"]\n",
    "    if allChinese(word) == False:\n",
    "        print(f\"word: {word}| at index: {index} is not all chinese\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9448139",
   "metadata": {},
   "source": [
    "Now, checking that definitions and words should be in the correct traditional or simplified format, checking using a special python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517cde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in new_data.iterrows():\n",
    "    #first convert characters \n",
    "    code = str(row[\"code\"])\n",
    "    word = str(row[\"word/character\"])\n",
    "    defi = str(row[\"definition\"])\n",
    "    \n",
    "    for char in word:\n",
    "        if code == \"s\" and contains_chinese(char): #run checks on all inerated \n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(f\"at word: {word} ({index})|{char} not simplified chinese\")\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(f\"at word: {word} ({index})|{char} not traditional chinese\")\n",
    "    for char in defi:\n",
    "        if code == \"s\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_simplified(char) == False:\n",
    "                print(f\"at def: {defi} ({index})|{char} not simplified chinese\")\n",
    "        if code == \"t\" and contains_chinese(char):\n",
    "            if hanzidentifier.is_traditional(char) == False:\n",
    "                print(f\"at def: {defi} ({index})|{char} not traditional chinese\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b412d",
   "metadata": {},
   "source": [
    "Now, replace (F, (T substrings in the definition column. done with find and replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ffb04",
   "metadata": {},
   "source": [
    "Saving after manual repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = new_data.columns[new_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "new_data.drop(columns=unnamed_cols, inplace=True)\n",
    "new_data = new_data.reindex()\n",
    "new_data.to_csv(path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70c5be",
   "metadata": {},
   "source": [
    "### Part 4: create specialized datasets for final export to json for a dataset of characters and words with unique characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88a157",
   "metadata": {},
   "source": [
    "Importing helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getbetween(input, start, end): #helper function to get the string in between two symbols (the first matching abtter)\n",
    "    startind = input.find(start)\n",
    "    #if the first one is not found, return empty:\n",
    "    if startind == -1: \n",
    "        return \"\"\n",
    "    #narrow down input\n",
    "    input = input[startind+len(start):]\n",
    "    endind = input.find(end)\n",
    "    #if the first one is not found (but start is found), return the string starting from the end \n",
    "    if endind == -1:\n",
    "        return \"\"\n",
    "    return input[:endind] #otherwise, return \n",
    "\n",
    "#create function check if a string of chracters is all chinese\n",
    "def contains_chinese(text): \n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]'  \n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "#create function check if a single characer is all chinese or punctuation\n",
    "def char_chinese_or_punctuation(text): \n",
    "    if len(text) < 1:\n",
    "        print(\"must be a single string character\")\n",
    "        return False\n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]' \n",
    "    isChinese = bool(re.search(pattern, text))\n",
    "    isPronuncation = text in string.punctuation\n",
    "    return (isChinese or isPronuncation)\n",
    "\n",
    "def allChinese(text):\n",
    "    for char in text: \n",
    "        if char_chinese_or_punctuation(char) == False: #use function to check if a single characer is chiense\n",
    "            return False \n",
    "    return True\n",
    "\n",
    "#helper function, that find the help of a map from Traditional character in a string to simplified, attempts conversion of a string\n",
    "def toSimplified(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                rettext += map[char]\n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"trad char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext\n",
    "\n",
    "#helper function, that find the help of our custom map a map from simplified characters to traditional, attempts conversion of a string\n",
    "#handles cases where there are multiple mappings\n",
    "def toTraditional(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                #check the mapped list\n",
    "                if len(map[char]) == 1:\n",
    "                    rettext += map[char][0]\n",
    "                else: \n",
    "                    print(f\"simp char {char} has multiple possible traditional chars, use manual mapping, using the same character as the original text\")\n",
    "                    rettext += text[i] \n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"simp char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext\n",
    "\n",
    "#helper function determine if a character has diacritic or not\n",
    "def contains_diacritic(char):\n",
    "  nfkd_form = unicodedata.normalize('NFKD', char)  # Normalize to decompose characters\n",
    "  for x in nfkd_form:\n",
    "    if unicodedata.combining(x):  # Check if the character is a combining character\n",
    "      return True\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f169d30",
   "metadata": {},
   "source": [
    "checking new helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d15ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contains_diacritic(\"zhǎng\"))\n",
    "print(contains_diacritic(\"zhǎng\"))\n",
    "print(contains_diacritic(\"Equivalent\"))\n",
    "print(contains_diacritic(\"{Compare with 方\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba3045",
   "metadata": {},
   "source": [
    "#### subpart 1:\n",
    "\n",
    "Create our final character dataset (also includes multi-character words that have a single unique character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a reference for current unique single characters \n",
    "unique_characters = pd.read_csv(\"./character_list.csv\")\n",
    "unique_characters = unique_characters[\"word/character\"].unique()\n",
    "print(len(unique_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"./main_final_list.csv\")\n",
    "cats = new_data[\"cat\"].unique() #get the unique categories  \n",
    "keep = [] #index of values to keep\n",
    "#iterate based on category\n",
    "for cat in cats:\n",
    "    #get all characters \n",
    "    subset = new_data[new_data[\"cat\"] == cat]\n",
    "    subset = subset.drop_duplicates(subset=['word/character', \"pronunciation\", \"code\"]) #drop based on the same character, pronunciation, and code\n",
    "    #iterate through the subset\n",
    "    for index, row in subset.iterrows():\n",
    "        #first convert characters \n",
    "        code = str(row[\"code\"])\n",
    "        word = str(row[\"word/character\"])\n",
    "        defi = str(row[\"definition\"])\n",
    "\n",
    "        #if the word is a single character, we keep this index\n",
    "        if len(word) == 1:\n",
    "            keep.append(index)\n",
    "        else: #else interate through all characters (assuming it is one)\n",
    "            for char in word: #if the character is chinese, and it is not in the set of chinese characters, we can keep these indices \n",
    "                if contains_chinese(char) == True and (char not in unique_characters):\n",
    "                    keep.append(index)\n",
    "\n",
    "#testable character dataset:\n",
    "test_data = new_data.loc[keep] \n",
    "test_data = test_data.reindex()\n",
    "print(len(test_data))\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data.to_csv(\"./testable_characters_dataset.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ddcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reindex again by reading in the same csv file \n",
    "test_link = \"./testable_characters_dataset.csv\"\n",
    "test_data = pd.read_csv(test_link)\n",
    "test_data = test_data.reindex()\n",
    "print(len(test_data))\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data.to_csv(test_link, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57984",
   "metadata": {},
   "source": [
    "This dataset will be the set of characters and small words tested. For the compound words, we now attempt to get their pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06934a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./testable_characters_dataset.csv\")\n",
    "test_data[\"full_pronunciation\"] = \"x\" #add a column for the full pronunciation\n",
    "for index, row in test_data.iterrows():\n",
    "    #first convert characters \n",
    "    code = str(row[\"code\"])\n",
    "    word = str(row[\"word/character\"])\n",
    "    defi = str(row[\"definition\"])\n",
    "    charpro = str(row[\"pronunciation\"])\n",
    "\n",
    "    #if the word is a single character the actual pronuciation is trivially, this is the  actual one\n",
    "    lw = len(word)\n",
    "    if lw == 1:\n",
    "        test_data.at[index, \"full_pronunciation\"] = charpro\n",
    "    elif lw >= 2:\n",
    "        print(\"compound word:\", word)\n",
    "        #iterate through the definition (it should be there). words by find the first and last char by diacritic\n",
    "        #clear chinese characters in the definitions and trim\n",
    "        for char in defi: \n",
    "            if contains_chinese(char): #find if the character is chinese \n",
    "                defi = defi.replace(char, \"\")\n",
    "        defi = defi.strip()\n",
    "        #split the definition by space\n",
    "        defi_split = defi.split(\" \")\n",
    "        #the pronuciation should be the 1st thing\n",
    "        full_pro =  defi_split[0]\n",
    "        #save pronucniation\n",
    "        test_data.at[index, \"full_pronunciation\"] = full_pro\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(test_link, index=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2a56c",
   "metadata": {},
   "source": [
    "Checking manually to see if pronuciations for 2 character words are valid/filled, a few changes were made, but most were correct. We save to a new csv file (final_test_characters) to prevent accidental overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test reading in \n",
    "\n",
    "test_data = pd.read_csv(\"final_test_characters.csv\")\n",
    "for index, row in test_data.iterrows(): \n",
    "    #first convert characters \n",
    "    code = str(row[\"code\"])\n",
    "    word = str(row[\"word/character\"])\n",
    "    defi = str(row[\"definition\"])\n",
    "    full_pro = str(row[\"full_pronunciation\"])      \n",
    "    if len(word) >= 2:\n",
    "        print(f\"found pronunciation {full_pro} for index: {index}, for word: {word}\")#if we have a diacritic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452e840",
   "metadata": {},
   "source": [
    "Now, add a column without diacritics (this will be useful for one of the user's test versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(input_str): #helper function to remove diacritcis\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "# Example usage:\n",
    "text_with_diacritics = \"Crème brûlée is delicious!\"\n",
    "text_without_diacritics = remove_diacritics(text_with_diacritics)\n",
    "print(text_without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = pd.read_csv(\"final_test_characters.csv\")\n",
    "test_data[\"full_pronunciation_wo\"] = \"None\"\n",
    "for index, row in test_data.iterrows(): \n",
    "    full_pro = str(row[\"full_pronunciation\"])      \n",
    "    test_data.at[index, \"full_pronunciation_wo\"] = remove_diacritics(full_pro)\n",
    "\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(\"final_test_characters.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e56896",
   "metadata": {},
   "source": [
    "### Part 5: Final stage for single characters\n",
    "\n",
    "For now, we are done with painstaking manual changes to the characters data (final_test_characters.csv), and now get ready to export our data in a JSON format. First, however, we create a test version of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03212c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"final_test_characters.csv\" #\"final_test_characters.csv\"\n",
    "test_data = pd.read_csv(path)\n",
    "test_data[\"test_definition\"]=None\n",
    "for index, row in test_data.iterrows(): \n",
    "    defi = str(row[\"definition\"])      \n",
    "    pro = str(row[\"full_pronunciation\"])   #get the pronuciation. Replace all instances with _ for the test definition\n",
    "    rettext = \"\" #set to return\n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in defi:\n",
    "        #if the chracter is chinese, we skip (essenially skipping it)\n",
    "        if contains_chinese(char) == True:\n",
    "            continue\n",
    "        else:\n",
    "            rettext += char\n",
    "        #for each substring we build, we see if we can find any  {} enclosed text, and these other combinations\n",
    "        start = \"{\"\n",
    "        end = \"}\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\") #works even if nothing is found\n",
    "\n",
    "        start = \"(Equivalent:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\") #works even if nothing is found\n",
    "\n",
    "        start = \"(Traditional:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\")\n",
    "\n",
    "        start = \"(Simplified:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\")\n",
    "\n",
    "        i+=1#in this case we do not change =\n",
    "    rettext = rettext.replace(pro, \"_\")\n",
    "    test_data.at[index, \"test_definition\"] = rettext.strip() #change definition\n",
    "\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54032c6a",
   "metadata": {},
   "source": [
    "Check subset of data with empty test_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1083e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = test_data[test_data[\"test_definition\"] == \"\"]\n",
    "print(len(subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.head(n=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f82262",
   "metadata": {},
   "source": [
    "Drop these characters, then with no clear definition beside the extra infromation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data[\"test_definition\"] != \"\"]\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(path, index=True)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaafa24",
   "metadata": {},
   "source": [
    "reindex again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"final_test_characters.csv\" #\"final_test_characters.csv\"\n",
    "test_data = pd.read_csv(path)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5d3ca",
   "metadata": {},
   "source": [
    "Checking for rows where created rows test_definition or full_pronunciation_wo, full_pronunciation are missing. More manual repairs were done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"final_test_characters.csv\" #\"final_test_characters.csv\"\n",
    "test_data = pd.read_csv(path, keep_default_na=False) #do not interpret nan pronunciation an null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec44193",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = test_data[test_data[\"full_pronunciation\"].isna()]\n",
    "print(len(subset))\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = test_data[test_data[\"full_pronunciation_wo\"].isna()]\n",
    "print(len(subset))\n",
    "subset.head(len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1ce7f",
   "metadata": {},
   "source": [
    "It appears all cases are because of nan characters with a nan pronunciation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8131d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = test_data[test_data[\"full_pronunciation\"].isna()]\n",
    "print(len(subset))\n",
    "subset.head(len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434a53d",
   "metadata": {},
   "source": [
    "Create our json (both a simplified and traditional version). This will be a dictionary of a list of dictionaries mapping to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6c00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e85e2",
   "metadata": {},
   "source": [
    "#### finally, save to JSON, dropping unnamed columns in the dataframe before converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8763f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cat</th>\n",
       "      <th>word/character</th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>definition</th>\n",
       "      <th>code</th>\n",
       "      <th>alt</th>\n",
       "      <th>full_pronunciation</th>\n",
       "      <th>full_pronunciation_wo</th>\n",
       "      <th>test_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "      <td>s</td>\n",
       "      <td></td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>dí</td>\n",
       "      <td>true, real</td>\n",
       "      <td>s</td>\n",
       "      <td></td>\n",
       "      <td>dí</td>\n",
       "      <td>di</td>\n",
       "      <td>true, real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>dí</td>\n",
       "      <td>true, real</td>\n",
       "      <td>t</td>\n",
       "      <td></td>\n",
       "      <td>dí</td>\n",
       "      <td>di</td>\n",
       "      <td>true, real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>一</td>\n",
       "      <td>yī</td>\n",
       "      <td>one, a little</td>\n",
       "      <td>s</td>\n",
       "      <td>壹</td>\n",
       "      <td>yī</td>\n",
       "      <td>yi</td>\n",
       "      <td>one, a little</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  cat word/character pronunciation  \\\n",
       "0           0    1              的            de   \n",
       "1           1    1              的            de   \n",
       "2           2    1              的            dí   \n",
       "3           3    1              的            dí   \n",
       "4           4    2              一            yī   \n",
       "\n",
       "                                          definition code alt  \\\n",
       "0  <grammatical particle marking genitive as well...    s       \n",
       "1  <grammatical particle marking genitive as well...    t       \n",
       "2                                         true, real    s       \n",
       "3                                         true, real    t       \n",
       "4                                      one, a little    s   壹   \n",
       "\n",
       "  full_pronunciation full_pronunciation_wo  \\\n",
       "0                 de                    de   \n",
       "1                 de                    de   \n",
       "2                 dí                    di   \n",
       "3                 dí                    di   \n",
       "4                 yī                    yi   \n",
       "\n",
       "                                     test_definition  \n",
       "0  <grammatical particle marking genitive as well...  \n",
       "1  <grammatical particle marking genitive as well...  \n",
       "2                                         true, real  \n",
       "3                                         true, real  \n",
       "4                                      one, a little  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"final_test_characters.csv\" #\"final_test_characters.csv\"\n",
    "test_data = pd.read_csv(path, keep_default_na=False) #do not interpret nan pronunciation an null value\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8de0b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>word/character</th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>definition</th>\n",
       "      <th>code</th>\n",
       "      <th>alt</th>\n",
       "      <th>full_pronunciation</th>\n",
       "      <th>full_pronunciation_wo</th>\n",
       "      <th>test_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "      <td>s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>&lt;grammatical particle marking genitive as well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>dí</td>\n",
       "      <td>true, real</td>\n",
       "      <td>s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dí</td>\n",
       "      <td>di</td>\n",
       "      <td>true, real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>的</td>\n",
       "      <td>dí</td>\n",
       "      <td>true, real</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dí</td>\n",
       "      <td>di</td>\n",
       "      <td>true, real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>一</td>\n",
       "      <td>yī</td>\n",
       "      <td>one, a little</td>\n",
       "      <td>s</td>\n",
       "      <td>壹</td>\n",
       "      <td>yī</td>\n",
       "      <td>yi</td>\n",
       "      <td>one, a little</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat word/character pronunciation  \\\n",
       "0    1              的            de   \n",
       "1    1              的            de   \n",
       "2    1              的            dí   \n",
       "3    1              的            dí   \n",
       "4    2              一            yī   \n",
       "\n",
       "                                          definition code  alt  \\\n",
       "0  <grammatical particle marking genitive as well...    s  NaN   \n",
       "1  <grammatical particle marking genitive as well...    t  NaN   \n",
       "2                                         true, real    s  NaN   \n",
       "3                                         true, real    t  NaN   \n",
       "4                                      one, a little    s    壹   \n",
       "\n",
       "  full_pronunciation full_pronunciation_wo  \\\n",
       "0                 de                    de   \n",
       "1                 de                    de   \n",
       "2                 dí                    di   \n",
       "3                 dí                    di   \n",
       "4                 yī                    yi   \n",
       "\n",
       "                                     test_definition  \n",
       "0  <grammatical particle marking genitive as well...  \n",
       "1  <grammatical particle marking genitive as well...  \n",
       "2                                         true, real  \n",
       "3                                         true, real  \n",
       "4                                      one, a little  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = test_data.loc[:, ~test_data.columns.str.startswith('Unnamed:')] #no unamed columns\n",
    "test_data.to_csv(path)\n",
    "test_data = pd.read_csv(path, index_col=0)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cca6de",
   "metadata": {},
   "source": [
    "Start with traditional single characters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb237d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start \n",
    "ourdict = {}\n",
    "our_data = test_data[test_data[\"code\"] == \"t\"]\n",
    "\n",
    "json_index = our_data.to_json(orient='index')\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce5b3e",
   "metadata": {},
   "source": [
    "This JSON is copied to the pages/data part of the webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbdff6",
   "metadata": {},
   "source": [
    "Finally, get the same data for the simplified characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e451311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start \n",
    "ourdict = {}\n",
    "our_data = test_data[test_data[\"code\"] == \"s\"]\n",
    "cats = our_data[\"cat\"].unique() #get the unique categories  \n",
    "json_index = our_data.to_json(orient='index')\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444ad5f",
   "metadata": {},
   "source": [
    "Manually replaced NaN string with null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be03a3aa",
   "metadata": {},
   "source": [
    "### Part 6: Final stage for words (multi-character words)- create the csv dataset of multi-character words (test_multi_list.csv) with test definitions to allow definitions of words to be tested. This csv is finally exported to JSON, in a format that is actually used by our aplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac796bc",
   "metadata": {},
   "source": [
    "Define helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5987ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getbetween(input, start, end): #helper function to get the string in between two symbols (the first matching abtter)\n",
    "    startind = input.find(start)\n",
    "    #if the first one is not found, return empty:\n",
    "    if startind == -1: \n",
    "        return \"\"\n",
    "    #narrow down input\n",
    "    input = input[startind+len(start):]\n",
    "    endind = input.find(end)\n",
    "    #if the first one is not found (but start is found), return the string starting from the end \n",
    "    if endind == -1:\n",
    "        return \"\"\n",
    "    return input[:endind] #otherwise, return \n",
    "\n",
    "#create function check if a string of chracters is all chinese\n",
    "def contains_chinese(text): \n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]'  \n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "#create function check if a single characer is all chinese or punctuation\n",
    "def char_chinese_or_punctuation(text): \n",
    "    if len(text) < 1:\n",
    "        print(\"must be a single string character\")\n",
    "        return False\n",
    "    pattern = r'[\\u4e00-\\u9fff\\u3400-\\u4dbf]' \n",
    "    isChinese = bool(re.search(pattern, text))\n",
    "    isPronuncation = text in string.punctuation\n",
    "    return (isChinese or isPronuncation)\n",
    "\n",
    "def allChinese(text):\n",
    "    for char in text: \n",
    "        if char_chinese_or_punctuation(char) == False: #use function to check if a single characer is chiense\n",
    "            return False \n",
    "    return True\n",
    "\n",
    "#helper function, that find the help of a map from Traditional character in a string to simplified, attempts conversion of a string\n",
    "def toSimplified(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                rettext += map[char]\n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"trad char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext\n",
    "\n",
    "#helper function, that find the help of our custom map a map from simplified characters to traditional, attempts conversion of a string\n",
    "#handles cases where there are multiple mappings\n",
    "def toTraditional(text, map):\n",
    "    rettext = \"\" \n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in str(text):\n",
    "        #if the chracter is chiense \n",
    "        if contains_chinese(char) == True:\n",
    "            #check if the character is in the map\n",
    "            if char in map:\n",
    "                #check the mapped list\n",
    "                if len(map[char]) == 1:\n",
    "                    rettext += map[char][0]\n",
    "                else: \n",
    "                    print(f\"simp char {char} has multiple possible traditional chars, use manual mapping, using the same character as the original text\")\n",
    "                    rettext += text[i] \n",
    "            else: #print that the character could not be converted\n",
    "                print(f\"simp char {char} cannot be converted based our map, using the same character as the original text\")\n",
    "                rettext += text[i]\n",
    "        else: \n",
    "            rettext += text[i]\n",
    "        i+=1#in this case we do not change =\n",
    "    return rettext\n",
    "\n",
    "#helper function determine if a character has diacritic or not\n",
    "def contains_diacritic(char):\n",
    "  nfkd_form = unicodedata.normalize('NFKD', char)  # Normalize to decompose characters\n",
    "  for x in nfkd_form:\n",
    "    if unicodedata.combining(x):  # Check if the character is a combining character\n",
    "      return True\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb4b10",
   "metadata": {},
   "source": [
    "narrow down to words with more than 1 character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60666cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data \n",
    "path = \"main_final_list.csv\" #\"final_test_characters.csv\"\n",
    "test_data = pd.read_csv(path)\n",
    "print(len(test_data))\n",
    "test_data = test_data[test_data[\"word/character\"].str.len() > 1]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data[\"test_definition\"]=None\n",
    "for index, row in test_data.iterrows(): \n",
    "    defi = str(row[\"definition\"])      \n",
    "    pro = str(row[\"pronunciation\"])   #get the pronuciation. Replace all instances with _ for the test definition\n",
    "    rettext = \"\" #set to return\n",
    "    #keep track of string index\n",
    "    i = 0\n",
    "    for char in defi:\n",
    "        #if the chracter is chinese, we skip (essenially skipping it)\n",
    "        if contains_chinese(char) == True:\n",
    "            continue\n",
    "        else:\n",
    "            rettext += char\n",
    "        #for each substring we build, we see if we can find any  {} enclosed text, and these other combinations\n",
    "        start = \"{\"\n",
    "        end = \"}\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\") #works even if nothing is found\n",
    "\n",
    "        start = \"(Equivalent:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\") #works even if nothing is found\n",
    "\n",
    "        start = \"(Traditional:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\")\n",
    "\n",
    "        start = \"(Simplified:\"\n",
    "        end = \")\"\n",
    "        found_enclosed = getbetween(rettext, start,end) #returns \"\" \n",
    "        rettext=rettext.replace(start+found_enclosed+end, \"\")\n",
    "\n",
    "        i+=1#in this case we do not change =\n",
    "    rettext = rettext.replace(pro, \"_\")\n",
    "    test_data.at[index, \"test_definition\"] = rettext.strip() #change definition\n",
    "\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(\"test_multi_list.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4c4c7",
   "metadata": {},
   "source": [
    "Check for missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = test_data[test_data[\"test_definition\"] == \"\"]\n",
    "print(len(subset))\n",
    "subset.head(n=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data[\"test_definition\"] != \"\"]\n",
    "# Identify columns containing 'unnamed' (case-insensitive)\n",
    "unnamed_cols = test_data.columns[test_data.columns.str.contains('unnamed', case=False)]\n",
    "# Drop these columns\n",
    "test_data.drop(columns=unnamed_cols, inplace=True)\n",
    "test_data = test_data.reindex()\n",
    "test_data.to_csv(\"test_multi_list.csv\", index=True)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6893d3f",
   "metadata": {},
   "source": [
    "### Part 7: Saving words (multi-character words) data to final json data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7e200",
   "metadata": {},
   "source": [
    "traditional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data = test_data[test_data[\"code\"] == \"t\"]\n",
    "\n",
    "json_index = our_data.to_json(orient='index')\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8ea8a",
   "metadata": {},
   "source": [
    "simplified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b9b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_data = test_data[test_data[\"code\"] == \"s\"]\n",
    "\n",
    "json_index = our_data.to_json(orient='index')\n",
    "\n",
    "# Print the JSON string\n",
    "print(json_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
